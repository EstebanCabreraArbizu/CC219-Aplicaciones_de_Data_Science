{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EstebanCabreraArbizu/CC219-Aplicaciones_de_Data_Science/blob/main/S2-Introducci%C3%B3n%20a%20NLP/01_Uso_de_NLTK_en_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso de NLTK"
      ],
      "metadata": {
        "id": "fXjOv0sB_N8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SACapOdX9sQ8",
        "outputId": "386eda08-3364-412f-8148-c26088703621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "#descargar el corpus en español\n",
        "nltk.download(\"cess_esp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expresiones regulares"
      ],
      "metadata": {
        "id": "xTIBg5zG_UeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "\n",
        "print(corpus)\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85rav7DS9-z9",
        "outputId": "aad29f7f-028f-4a94-e1db-6fc555005b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplanar una lista compuesta de sublistas\n",
        "\n",
        "flatten = [w for l in corpus for w in l]\n"
      ],
      "metadata": {
        "id": "O0bP1_OCAs6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(flatten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_vp-Wq7BhVq",
        "outputId": "85feb771-1c07-4b60-fa72-7987ba3a7fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192686"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(flatten[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98m5FjA-Bq0d",
        "outputId": "01e9522d-d255-443d-9227-a4bcc4aac6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicacion de expresiones regulares sobre la variable **flatten**:\n",
        "\n",
        "re.search(p,s)\n",
        "\n",
        "p = patron de busqueda\n",
        "s = cadena donde buscar"
      ],
      "metadata": {
        "id": "k2rghBc8CFNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta-caracteres basicos\n",
        "\n",
        "arr = [w for w in flatten if re.search(\"es\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIjtvgOFCMX2",
        "outputId": "07f3fed5-4ab9-42c4-ddc4-a551c3a92f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = [w for w in flatten if re.search(\"es$\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rguHBFA1DcnO",
        "outputId": "be7edfcd-5e17-4ec3-dd36-ce0842aa65a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jueves', 'centrales', 'millones', 'millones', 'dólares']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = [w for w in flatten if re.search(\"^..j..t..$\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf0timi3DqWt",
        "outputId": "af9932bc-95dc-47d4-feb7-523d0ee5d9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tajantes']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicacion de rangos\n",
        "# [a-z], [A-Z], [0-9]\n",
        "arr = [w for w in flatten if re.search(\"^[ghi][mno][jlk][def]$\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEV2X6vLEUxR",
        "outputId": "62418afb-4c3e-4b5b-86eb-34e79f3b7d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['golf', 'golf']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizacion del Texto"
      ],
      "metadata": {
        "id": "oSAxM7TN88XV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Tokenizacion"
      ],
      "metadata": {
        "id": "oiR8tDZ-92Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\"Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
        "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\"\"\"\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zDPPIS-VCe",
        "outputId": "b482b8e8-f965-4c39-d44b-860a9b66ea48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
            "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso 1\n",
        "print(re.split(r' ', texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pz-deq_RSd",
        "outputId": "ac6a93b8-75b3-4f05-8ea8-95f105007576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.\\nEra', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso 2:\n",
        "print(re.split(r'[ \\t\\n]+', texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXCUO-_J_8Fo",
        "outputId": "a7eb415f-4446-4c4f-e277-b48433d13c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso 3:\n",
        "# \\W omite todos los caracteres que nos sean letras, digitos o guiones\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rd1GqRxAaVp",
        "outputId": "6c5d59ad-7e8a-48bb-f263-67b120a3809e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', 'imaginaba', 'en', 'mi', 'cabeza', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"En los E.U. esa hamburguesa cuesta $15.50\"\n",
        "print(re.split(r'[ \\W\\t\\n]+',texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JRxhZQ_9ARl",
        "outputId": "725db6fa-a989-4298-ece4-db732471d864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'hamburguesa', 'cuesta', '15', '50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'''(?x)                 # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
        "              | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
        "              | \\.\\.\\.             # ellipsis\n",
        "              | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''"
      ],
      "metadata": {
        "id": "JMFBeGqhBi5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.regexp_tokenize(texto, pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLYphGDNDCdS",
        "outputId": "351e538e-5a5b-48a7-b68e-7f3aeb6d730f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'hamburguesa', 'cuesta', '$15.50']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Lematizacion (encontrar la raiz linguistica de una palabra)\n",
        "\n",
        "Stemming = lematizacion simple\n",
        "* Lematizado != Estematizado"
      ],
      "metadata": {
        "id": "LqZ7vvrAEIAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WJG9u5EPlV",
        "outputId": "faa372b0-a129-4b4b-c1ae-188735ae6eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem = SnowballStemmer('spanish')"
      ],
      "metadata": {
        "id": "8NBaDQrFFHrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem.stem(\"trabajaremos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZYBKglCEFQC3",
        "outputId": "2d81b4ec-82e6-4df5-92e9-f3e4c7ce5909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trabaj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lematizacion\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemm= WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "oFLQxiP1Fqmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0PQVEVvF8Cl",
        "outputId": "46b20810-ab95-4846-f5bc-6c9bf5450182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemm.lemmatize('trabajó')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7jz9kHSgGK12",
        "outputId": "a0afd9cb-e86c-4695-94ef-ea827c1bd417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trabajó'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de Spacy en NLP"
      ],
      "metadata": {
        "id": "pjoF-eNiGn1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spacy** es una librería de procesamiento del lenguaje natural, robusta, rápida, fácil de instalar y utilizar e integrable con otras librerías de NLP y de deep learning.\n",
        "\n",
        "Tiene modelos entrenados en varios idiomas y permite realizar las típicas tareas de segmentación por oraciones, tokenizanción, análisis morfológico, extracción de entidades y análisis de opinión.\n",
        "\n",
        "Una vez instalados los modelos, podemos importarlos fácilmente:"
      ],
      "metadata": {
        "id": "RrU39xqKTR-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKYCTJ_8GyIs",
        "outputId": "9d1486d3-9e71-42a3-9fd6-41f77de4616e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos el modelo de Spacy en español\n",
        "!python -m spacy download es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVIJYU9eG7s0",
        "outputId": "0268f5d2-b063-4f4d-bb7e-6fb8d9921f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "OFe2jbIeHSos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "id": "gaCumO75HZYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Soy un texto.  Normalmente soy más largo y más grande.  Que no te engañe mi tamaño mayor a 10 palabras\"\n",
        "#Una palabra puede ser representado por un vector"
      ],
      "metadata": {
        "id": "YPUUszhzIneC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "1nvb-jGgI7BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos una lista con los tokens del texto\n",
        "tokens = [t.orth_ for t in doc]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UerA9zJA-s",
        "outputId": "3a811bca-8966-455e-ba49-e46a0da9214b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy',\n",
              " 'un',\n",
              " 'texto',\n",
              " '.',\n",
              " ' ',\n",
              " 'Normalmente',\n",
              " 'soy',\n",
              " 'más',\n",
              " 'largo',\n",
              " 'y',\n",
              " 'más',\n",
              " 'grande',\n",
              " '.',\n",
              " ' ',\n",
              " 'Que',\n",
              " 'no',\n",
              " 'te',\n",
              " 'engañe',\n",
              " 'mi',\n",
              " 'tamaño',\n",
              " 'mayor',\n",
              " 'a',\n",
              " '10',\n",
              " 'palabras']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limpiando el texto en Spacy**"
      ],
      "metadata": {
        "id": "gZPwEl6iJyar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitir:\n",
        "\n",
        "- Palabras comunes (y, o, ni, que)\n",
        "- Preposiciones (a, en, para, por, entre, otras)\n",
        "- Verbos (ser)\n",
        "- Las puntuaciones\n",
        "\n",
        "Todas aquellas palabras que no aportan un significado importante en el texto las denominaremos stopwords"
      ],
      "metadata": {
        "id": "IxdPFIOXKgrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_validos = [t.orth_ for t in doc if (not t.is_punct | t.is_stop) and t.orth_ != ' ']\n",
        "tokens_validos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmr2Sd5sKutb",
        "outputId": "06da3ce0-3188-4f73-ef32-5d6cf8a2afd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['texto', 'Normalmente', 'engañe', 'tamaño', '10', 'palabras']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizar texto en Spacy**"
      ],
      "metadata": {
        "id": "jXidc8udKmrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [t.lower() for t in tokens_validos if len(t)>3 and t.isalpha()]\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLra4LqmJ2pr",
        "outputId": "7c9f6091-f4b9-4891-91f8-de4441e9b115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['texto', 'normalmente', 'engañe', 'tamaño', 'palabras']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0B6zvU80O0ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Similitud semántica entre palabras, frases y documentos**\n",
        "\n",
        "spaCy permite calcular la similitud semántica entre cualquier par de objetos de tipo Doc, Span o Token.\n",
        "\n",
        "Ojo, La similitud semántica es un concepto algo subjetivo, pero en este caso se puede entender como la probabilidad de que dos palabras aparezcan en los mismos contextos."
      ],
      "metadata": {
        "id": "xr7CNuUcXlm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "a90n_8y3W_bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "fCwXNGTnXAzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#La distancia entre estos vectores sería coseno\n",
        "# Puede ser la distancia euclediana o manhatan\n",
        "\n",
        "# analizamos algunas colocaciones en inglés\n",
        "token1, _, token2 = nlp_en(\"cats and dogs\")\n",
        "token3, _, token4 = nlp_en(\"research and development\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUjA16sFYPWk",
        "outputId": "bc67f124-2e8e-4e6e-f79c-d5443d4073cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats vs dogs 0.34804314374923706\n",
            "research vs development 0.1919044852256775\n",
            "cats vs development 0.14609646797180176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-de2381c05edf>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "<ipython-input-36-de2381c05edf>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "<ipython-input-36-de2381c05edf>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ¿qué tal funciona en español?\n",
        "token1, _, token2 = nlp(\"perros y gatos\")\n",
        "token3, _, token4 = nlp(\"investigación y desarrollo\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yac1-xYYPkT",
        "outputId": "36787f51-e1fa-4192-9e68-c434706947bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perros vs gatos 0.6648734211921692\n",
            "investigación vs desarrollo 0.182033509016037\n",
            "perros vs desarrollo -0.03289707377552986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-a9925c3ae975>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "<ipython-input-37-a9925c3ae975>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "<ipython-input-37-a9925c3ae975>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_O5Df5_XMNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}